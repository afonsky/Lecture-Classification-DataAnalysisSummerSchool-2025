---
layout: center
---
# Обучение без учителя

---
layout: center
---
# Снижение размерности

---

# Снижение размерности: метод главных компонент

### **Метод главных компонент (PCA)** - линейный метод снижения размерности, который преобразует исходные признаки в новый набор признаков (главных компонентов), сортируя их по величине дисперсии.<br><br>

* Преимущества:
  - Уменьшение мультиколлинеарности
  	- Преобразует коррелированные признаки в набор независимых главных компонентов
  - Сжатие данных
  	- Часто позволяет сохранить большую часть информации в меньшем числе признаков

---

# Метод главных компонент

<br>
<div class="grid grid-cols-[7fr_3fr] gap-20">
<div>
<figure>
  <img src="/ISLP_figure_6.14.svg" style="width: 500px !important;">
  <figcaption style="color:#b3b3b3ff; font-size: 11px; position: relative; top: 10px; left: 40px;">Источник изображения:
    <a href="https://hastie.su.domains/ISLR2/ISLP_website.pdf#page=262">ISLP Fig. 6.14</a>
  </figcaption>
</figure>
</div>
<div>

#### PCA в sklearn
```python {}
from sklearn import datasets, decomposition
pca = decomposition.PCA(n_components=3)
pca.fit(X)
X = pca.transform(X)
```
</div>
</div>
<br>
<br>

#### Не забудьте провести стандартизацию признаков перед  использованием PCA!

[Демо](https://projector.tensorflow.org/)

---

# Выбор числа главных компонент

* Мы предпочитаем число главных компонент, которые объясняют **большую** изменчивость всех признаков $X_i$
	* Предполагается, что необъясненная изменчивость является результатом шума
* Для оценки часто используются графики типа Scree или Elbow
	* Ищите наибольший спад в доле объясненной дисперсии
		* Или для наибольшего роста накопленной доли объясненной дисперсии
* Это сжатие лучше всего работает для линейно связанных предикторов


<div class="grid grid-cols-[8fr_8fr] gap-10">
<div>
<br>

* Вы можете "сжать" тысячи признаков до десятка с незначительной "потерей информации"
</div>
<div>
<figure>
  <img src="/ISLP_figure_12.3.svg" style="width: 430px !important;">
  <figcaption style="color:#b3b3b3ff; font-size: 11px; position: relative; top: 10px; left: 230px;">Image source:
    <a href="https://hastie.su.domains/ISLR2/ISLP_website.pdf#page=518">ISLP Fig. 12.3</a>
  </figcaption>
</figure>
</div>
</div>

---

# Линейный Дискриминантный Анализ

### **Линейный Дискриминантный Анализ (LDA)** - линейный метод, который стремится найти линейные комбинации признаков, максимизирующие различие между классами.<br><br>

* Преимущества:
  - Улучшение различимости классов
  	* Хорошо работает для задач классификации
  - Уменьшение размерности
  	* cнижает размерность до $K-1$, где $K$ — количество классов

---

# t-распределенное стохастическое вложение соседей

### **t-распределенное стохастическое вложение соседей (t-SNE)** - нелинейный метод, направленный на визуализацию высокоразмерных данных путем моделирования вероятностей соседства точек.<br><br>

* Преимущества:
  - Визуализация
  	* Хорош для визуализации высокоразмерных данных в двумерном или трёхмерном пространстве
  - Учет сложных структур
  	* Способен сохранять сложные нелинейные структуры данных

[Демо](https://wedadanbtawi95.github.io/tsne/)

---

# Самоорганизующиеся карты

### **Самоорганизующиеся карты (SOM)** - нейронная сеть, обучающаяся отображать высокоразмерные данные в двумерное пространство, сохраняя топологические свойства данных.
 * Преимущества:
  - Топологическое упорядочение
  	* служит для кластеризации и визуализации данных
  - Интерпретируемость
  	* Карта легко интерпретируется, поскольку сохраняет топологию исходных данных

---

# Автокодировщики

### **Автокодировщики (автоэнкодеры)** - нейронные сети, обучающиеся кодировать входные данные в сжатое представление и затем восстанавливать исходные данные из этого представления.
* Преимущества:
  - Обучение существенных признаков
  	* способны выявлять скрытые, важные представления во входных данных
  - Гибкость
  	* применимы к сложным, нелинейным структурам данных
